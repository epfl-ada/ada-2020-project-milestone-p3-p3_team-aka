{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import wikipediaapi\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en')\n",
    "nlp_de = spacy.load('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_en = pd.read_csv('summaries_en.csv', index_col=0)\n",
    "summaries_de = pd.read_csv('summaries_de.csv', index_col=0).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_everything_in_parenth_or_brackets (text):\n",
    "    n = 1  \n",
    "    while n:\n",
    "        text1 = text\n",
    "        text, n = re.subn(r'\\([^()]*\\)', '', text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_en['summary_en'] = summaries_en['summary_en'].apply(remove_everything_in_parenth_or_brackets)\n",
    "summaries_de['summary_de'] = summaries_de['summary_en'].apply(remove_everything_in_parenth_or_brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_en['summary_en'] = summaries_en['summary_en'].apply(\\\n",
    "    lambda text: [token.lemma_.lower() for token in nlp_en(text) if not token.is_stop and not token.is_punct \\\n",
    "                                                and not token.is_space and not token.is_digit])\n",
    "\n",
    "summaries_de['summary_de'] = summaries_de['summary_de'].apply(\\\n",
    "    lambda text: [token.lemma_.lower() for token in nlp_de(text) if not token.is_stop and not token.is_punct \\\n",
    "                                                and not token.is_space and not token.is_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534    [t-12, cloudmaker, 20.000, kg, gro√ü, konventio...\n",
      "Name: summary_de, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(summaries_de.summary_de.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words_to_id(language):\n",
    "    language_to_embeddings_path = {'en': os.path.join(os.getcwd(), 'wiki.multi.en.vec'),\n",
    "                                   'de': os.path.join(os.getcwd(), 'wiki.multi.de.vec')}\n",
    "    path = language_to_embeddings_path[language]\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    nmax = 50000\n",
    "    with open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(tokens, word2id, embeddings):\n",
    "    num_words = 0\n",
    "    embedding = np.zeros(embeddings.shape[1], dtype='float')\n",
    "    for token in tokens:\n",
    "        if token in word2id.keys():\n",
    "            num_words += 1\n",
    "            embedding += embeddings[word2id[token]]\n",
    "\n",
    "    if num_words:\n",
    "        return (1.0 / num_words) * embedding\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_en, word2id_en = load_words_to_id('en')\n",
    "embeddings_de, word2id_de = load_words_to_id('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_embeddings_en = summaries_en['summary_en'].apply(lambda tokens: encode_tokens(tokens, word2id_en, embeddings_en)).dropna()\n",
    "article_embeddings_de = summaries_de['summary_de'].apply(lambda tokens: encode_tokens(tokens, word2id_de, embeddings_de)).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada-env] *",
   "language": "python",
   "name": "conda-env-ada-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
